{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeac372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes used: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'space']\n",
      "Number of classes: 28\n",
      "Total training samples: 67200\n",
      "Total validation samples: 8400\n",
      "Total test samples: 8400\n"
     ]
    }
   ],
   "source": [
    "import os, random, math\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "data_dir = \"./data/asl_alphabet_train/asl_alphabet_train/\" \n",
    "\n",
    "all_classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "\n",
    "exclude_classes = {\"nothing\"}\n",
    "classes = sorted([c for c in all_classes if c.lower() not in exclude_classes])\n",
    "\n",
    "print(\"Classes used:\", classes)\n",
    "print(\"Number of classes:\", len(classes)) \n",
    "\n",
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "\n",
    "for cls in classes:\n",
    "    class_dir = os.path.join(data_dir, cls)\n",
    "    files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    random.shuffle(files)\n",
    "    n_total = len(files)\n",
    "    n_train = math.floor(0.8 * n_total)   \n",
    "    n_val = math.floor(0.1 * n_total)    \n",
    "\n",
    "    train_list = files[:n_train]\n",
    "    val_list = files[n_train:n_train + n_val]\n",
    "    test_list = files[n_train + n_val:]\n",
    "    train_files += [(fp, cls) for fp in train_list]\n",
    "    val_files   += [(fp, cls) for fp in val_list]\n",
    "    test_files  += [(fp, cls) for fp in test_list]\n",
    "\n",
    "random.shuffle(train_files)\n",
    "random.shuffle(val_files)\n",
    "random.shuffle(test_files)\n",
    "\n",
    "print(f\"Total training samples: {len(train_files)}\")\n",
    "print(f\"Total validation samples: {len(val_files)}\")\n",
    "print(f\"Total test samples: {len(test_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e139954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746787693.496938  854308 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1746787693.549218  854395 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 545.23.06), renderer: NVIDIA GeForce RTX 2080 Ti/PCIe/SSE2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to index mapping: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'del': 26, 'space': 27}\n",
      "Processing training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/67200 [00:00<?, ?img/s]INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1746787693.602369  854347 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746787693.631705  854377 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746787693.644005  854361 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Train: 100%|██████████| 67200/67200 [48:20<00:00, 23.16img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val: 100%|██████████| 8400/8400 [06:04<00:00, 23.04img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 8400/8400 [06:03<00:00, 23.10img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted landmarks for training set: (49915, 21, 3)\n",
      "Example of landmarks for one image (first training sample):\n",
      "[[ 6.70948148e-01  4.44872350e-01  4.01725231e-08]\n",
      " [ 6.58323467e-01  4.50201988e-01 -1.04909234e-01]\n",
      " [ 5.98267555e-01  4.43777174e-01 -1.45214453e-01]\n",
      " [ 5.23130357e-01  4.57256496e-01 -1.67215347e-01]\n",
      " [ 4.56149369e-01  4.56595361e-01 -1.86277658e-01]\n",
      " [ 5.85710466e-01  2.89745212e-01 -1.10409737e-01]\n",
      " [ 4.92681563e-01  2.27920890e-01 -1.55761063e-01]\n",
      " [ 4.24179524e-01  1.95349589e-01 -1.84811428e-01]\n",
      " [ 3.68203580e-01  1.77432194e-01 -2.01348335e-01]\n",
      " [ 5.50077617e-01  3.00024092e-01 -7.32422173e-02]\n",
      " [ 4.47823346e-01  2.90391743e-01 -1.33259296e-01]\n",
      " [ 4.22418714e-01  3.55146646e-01 -1.60876855e-01]\n",
      " [ 4.28664953e-01  4.07414615e-01 -1.66381136e-01]\n",
      " [ 5.21646440e-01  3.19321662e-01 -4.44201268e-02]\n",
      " [ 4.30239826e-01  3.04076105e-01 -1.04157694e-01]\n",
      " [ 4.16438460e-01  3.65684658e-01 -1.23959482e-01]\n",
      " [ 4.27218378e-01  4.09238696e-01 -1.20582022e-01]\n",
      " [ 5.04539371e-01  3.38519245e-01 -2.29097884e-02]\n",
      " [ 4.34889734e-01  3.17986548e-01 -6.84589446e-02]\n",
      " [ 4.11526203e-01  3.40606630e-01 -8.37278739e-02]\n",
      " [ 4.10159826e-01  3.66217375e-01 -8.25956240e-02]]\n",
      "Saved training set to disk.\n",
      "Saved validation set to disk.\n",
      "Saved test set to disk.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True,  \n",
    "                       max_num_hands=1,        \n",
    "                       min_detection_confidence=0.5)\n",
    "\n",
    "def extract_landmarks(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None  \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        landmark_coords = []\n",
    "        for lm in hand_landmarks.landmark:\n",
    "            landmark_coords.append([lm.x, lm.y, lm.z])\n",
    "        return np.array(landmark_coords, dtype=np.float32)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "X_train, y_train = [], []\n",
    "X_val, y_val = [], []\n",
    "X_test, y_test = [], []\n",
    "\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "print(\"Class to index mapping:\", class_to_idx)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Processing training set...\")\n",
    "for filepath, cls in tqdm(train_files, desc=\"Train\", unit=\"img\"):\n",
    "    landmarks = extract_landmarks(filepath)\n",
    "    if landmarks is not None:\n",
    "        X_train.append(landmarks)\n",
    "        y_train.append(class_to_idx[cls])\n",
    "\n",
    "print(\"Processing validation set...\")\n",
    "for filepath, cls in tqdm(val_files, desc=\"Val\", unit=\"img\"):\n",
    "    landmarks = extract_landmarks(filepath)\n",
    "    if landmarks is not None:\n",
    "        X_val.append(landmarks)\n",
    "        y_val.append(class_to_idx[cls])\n",
    "\n",
    "print(\"Processing test set...\")\n",
    "for filepath, cls in tqdm(test_files, desc=\"Test\", unit=\"img\"):\n",
    "    landmarks = extract_landmarks(filepath)\n",
    "    if landmarks is not None:\n",
    "        X_test.append(landmarks)\n",
    "        y_test.append(class_to_idx[cls])\n",
    "\n",
    "X_train = np.array(X_train)  \n",
    "X_val   = np.array(X_val)  \n",
    "X_test  = np.array(X_test)   \n",
    "y_train = np.array(y_train)\n",
    "y_val   = np.array(y_val)\n",
    "y_test  = np.array(y_test)\n",
    "\n",
    "print(\"Extracted landmarks for training set:\", X_train.shape)\n",
    "print(\"Example of landmarks for one image (first training sample):\")\n",
    "print(X_train[0])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Save to disk\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "print(\"Saved training set to disk.\")\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save(\"y_val.npy\", y_val)\n",
    "print(\"Saved validation set to disk.\")\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "print(\"Saved test set to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d6784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved random test array to disk.\n"
     ]
    }
   ],
   "source": [
    "random_test_array = np.random.rand(21, 3).astype(np.float32)\n",
    "np.save(\"random_test_array.npy\", random_test_array)\n",
    "print(\"Saved random test array to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b65593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASLClassifier(\n",
      "  (conv1): Conv1d(3, 32, kernel_size=(3,), stride=(1,))\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=192, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=28, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train).permute(0, 2, 1)  \n",
    "X_val_tensor   = torch.tensor(X_val).permute(0, 2, 1)    \n",
    "X_test_tensor  = torch.tensor(X_test).permute(0, 2, 1)   \n",
    "y_train_tensor = torch.tensor(y_train).long()\n",
    "y_val_tensor   = torch.tensor(y_val).long()\n",
    "y_test_tensor  = torch.tensor(y_test).long()\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset   = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ASLClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ASLClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=3)   \n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)  \n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)  \n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)  \n",
    "        self.fc1 = nn.Linear(64 * 3, 128) \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, len(classes)) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  \n",
    "        return x\n",
    "\n",
    "model = ASLClassifier()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa666768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/10: Train Loss = 1.8466, Train Acc = 0.4361 | Val Loss = 0.6125, Val Acc = 0.8482\n",
      "Epoch 2/10: Train Loss = 0.6069, Train Acc = 0.8265 | Val Loss = 0.3550, Val Acc = 0.9271\n",
      "Epoch 3/10: Train Loss = 0.4346, Train Acc = 0.8836 | Val Loss = 0.2661, Val Acc = 0.9497\n",
      "Epoch 4/10: Train Loss = 0.3550, Train Acc = 0.9086 | Val Loss = 0.2484, Val Acc = 0.9492\n",
      "Epoch 5/10: Train Loss = 0.3124, Train Acc = 0.9190 | Val Loss = 0.1868, Val Acc = 0.9623\n",
      "Epoch 6/10: Train Loss = 0.2738, Train Acc = 0.9284 | Val Loss = 0.1678, Val Acc = 0.9656\n",
      "Epoch 7/10: Train Loss = 0.2506, Train Acc = 0.9363 | Val Loss = 0.1529, Val Acc = 0.9620\n",
      "Epoch 8/10: Train Loss = 0.2290, Train Acc = 0.9398 | Val Loss = 0.1404, Val Acc = 0.9691\n",
      "Epoch 9/10: Train Loss = 0.2108, Train Acc = 0.9449 | Val Loss = 0.1296, Val Acc = 0.9688\n",
      "Epoch 10/10: Train Loss = 0.1962, Train Acc = 0.9485 | Val Loss = 0.1156, Val Acc = 0.9715\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)            \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward()                 \n",
    "        optimizer.step()               \n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_sum += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    val_loss = val_loss_sum / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "          f\"Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f} | \"\n",
    "          f\"Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a2085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.9704\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "test_acc = test_correct / test_total\n",
    "print(f\"Test Accuracy = {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393419bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to asl_classifier_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"asl_classifier_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved to\", model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmep-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
